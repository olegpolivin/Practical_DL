{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqtXrZApBkM4"
   },
   "source": [
    "# Optimizing training and inference\n",
    "\n",
    "In this notebook, we will discuss different ways to reduce memory and compute usage during training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEt8wg4JCQdm"
   },
   "source": [
    "## Prepare training script (1 point)\n",
    "\n",
    "When training large models, it is usually a best practice not to use Jupyter notebooks, but run a **separate script** for training which could have command-line flags for various hyperparameters and training modes. This is especially useful when you need to run multiple experiments simultaneously (e.g. on a cluster with task scheduler). Another advantage of this is that after training, the process will finish and free the resources for other users of a shared GPU.\n",
    "\n",
    "In this part, you will need to put all your code to train a model on Tiny ImageNet that you wrote for the previous task in `train.py`.\n",
    "\n",
    "You can then run your script from inside of this notebook like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-TWiKq8H9yT",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 112.016s\n",
      "  (added) training part takes 103.935s to complete\n",
      "  (added) on average one batch of size 64s is processed in 0.035s\n",
      "  training loss (in-iteration): \t4.661176\n",
      "  validation accuracy: \t\t\t14.11 %\n",
      "Evaluating on test data\n",
      "Final results:\n",
      "  test accuracy:\t\t15.28 %\n",
      "We need more magic! Follow instructons below\n",
      "Peak memory usage by Pytorch tensors: 3064.78 Mb\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --bs=64 --num_epochs=1 --evaluate --sched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** \n",
    "\n",
    "Write code for training with architecture from homework_part2\n",
    "\n",
    "**Requirements**\n",
    "* Optional arguments from command line such as batch size and number of epochs with built-in argparse\n",
    "* Modular structure - separate functions for creating data generator, building model and training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKPYZ3QLEqX8"
   },
   "source": [
    "## Profiling time (1 point)\n",
    "\n",
    "For the next tasks, you need to add measurements to your training loop. You can use [`perf_counter`](https://docs.python.org/3/library/time.html#time.perf_counter) for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSr-PyQNFkSC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HMJMCGRKFYCc",
    "outputId": "571046a2-443b-465f-ce62-ddaf68b105d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication took %.3f seconds\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(1000, 1000)\n",
    "y = np.random.randn(1000, 1000)\n",
    "\n",
    "start_counter = time.perf_counter()\n",
    "z = x @ y\n",
    "elapsed_time = time.perf_counter() - start_counter\n",
    "print(\"Matrix multiplication took %.3f seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfhLeWjTGTpB"
   },
   "source": [
    "**Task**. You need to add the following measurements to your training script:\n",
    "* How much time a forward-backward pass takes for a single batch;\n",
    "* How much time an epoch takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khDOTn_SHaND"
   },
   "source": [
    "## Profiling memory usage (1 point)\n",
    "\n",
    "**Task**. You need to measure the memory consumptions\n",
    "\n",
    "This section depends on whether you train on CPU or GPU.\n",
    "\n",
    "### If you train on CPU\n",
    "You can use GNU time to measure peak RAM usage of a script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98xvXSjUIDzl"
   },
   "outputs": [],
   "source": [
    "!/usr/bin/time -lp python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1ES2Pc9IlH5"
   },
   "source": [
    "**Maximum resident set size**  will show you the peak RAM usage in bytes after the script finishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. \n",
    "Imports also require memory, do the correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kq5lY5CKJHX1"
   },
   "source": [
    "### If you train on GPU\n",
    "\n",
    "Use [`torch.cuda.max_memory_allocated()`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.max_memory_allocated) at the end of your script to show the maximum amount of memory in bytes used by all tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fSQdauqLIkf1",
    "outputId": "8bcffc30-637d-461a-8f44-0e444a28caae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak memory usage by Pytorch tensors: 3814.70 Mb\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1000, 1000, 1000, device='cuda:0')\n",
    "print(f\"Peak memory usage by Pytorch tensors: {(torch.cuda.max_memory_allocated() / 1024 / 1024):.2f} Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3RWHxYKBUys"
   },
   "source": [
    "## Gradient based techniques\n",
    "\n",
    "Modern architectures can potentially consume lots and lots of memory even for minibatch of several objects. To handle such cases here we will discuss two simple techniques.\n",
    "\n",
    "### Gradient Checkpointing (3 points)\n",
    "\n",
    "Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.\n",
    "\n",
    "See [blogpost](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9) for kind introduction and different strategies or [article](https://arxiv.org/pdf/1604.06174.pdf) for not kind introduction.\n",
    "\n",
    "**Task**. Use [built-in checkpointing](https://pytorch.org/docs/stable/checkpoint.html), measure the difference in memory/compute \n",
    "\n",
    "**Requirements**. \n",
    "* Try several arrangements for checkpoints\n",
    "* Add the chekpointing as the optional flag into your script\n",
    "* Measure the difference in memory/compute between the different arrangements and baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of explanation: I created various arrangements in `train.py` script in the `CheckpointedModel` class. I was not sure whether I should present them as special cases and run some `if -> else` statements. So that's what I did. And arrangements differ in that I gradually add `checkpoint_sequential` to the network's blocks.\n",
    "\n",
    "Why do I speak about `blocks` of the network? According to what I could find in the internet, checkpoints do not work properly with `BatchNorm` and `Dropout` layers. So what I did instead is to separate the network into blocks: each block does not have `batchnorm` or `dropout` layer, so each block represents what comes before and after those two special layers.\n",
    "\n",
    "Below I present my results in a pandas table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 113.201s\n",
      "  (added) training part takes 104.612s to complete\n",
      "  (added) on average one batch of size 128s is processed in 0.012s\n",
      "  training loss (in-iteration): \t4.679394\n",
      "  validation accuracy: \t\t\t14.19 %\n",
      "Evaluating on test data\n",
      "Final results:\n",
      "  test accuracy:\t\t14.74 %\n",
      "We need more magic! Follow instructons below\n",
      "Peak memory usage by Pytorch tensors: 5651.73 Mb\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --bs=128 --num_epochs=1 --evaluate --sched # Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 113.525s\n",
      "  (added) training part takes 104.935s to complete\n",
      "  (added) on average one batch of size 128s is processed in 0.012s\n",
      "  training loss (in-iteration): \t4.648259\n",
      "  validation accuracy: \t\t\t15.60 %\n",
      "Evaluating on test data\n",
      "Final results:\n",
      "  test accuracy:\t\t16.01 %\n",
      "We need more magic! Follow instructons below\n",
      "Peak memory usage by Pytorch tensors: 5651.73 Mb\n"
     ]
    }
   ],
   "source": [
    "# Arrangement 0 is the baseline model without checkpointed, but that uses `block structure`,\n",
    "# that is it is the model without checkpoints, but that is constructed using the CheckpointedModel class.\n",
    "# Expectation is that it perform the same time and memory usage as the baseline one\n",
    "!python3 train.py --bs=128 --num_epochs=1 --evaluate --sched --arrangement_num=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 113.517s\n",
      "  (added) training part takes 104.951s to complete\n",
      "  (added) on average one batch of size 128s is processed in 0.012s\n",
      "  training loss (in-iteration): \t4.666649\n",
      "  validation accuracy: \t\t\t14.17 %\n",
      "Evaluating on test data\n",
      "Final results:\n",
      "  test accuracy:\t\t15.17 %\n",
      "We need more magic! Follow instructons below\n",
      "Peak memory usage by Pytorch tensors: 5651.73 Mb\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --bs=128 --num_epochs=1 --evaluate --sched --arrangement_num=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 115.267s\n",
      "  (added) training part takes 106.662s to complete\n",
      "  (added) on average one batch of size 128s is processed in 0.013s\n",
      "  training loss (in-iteration): \t4.660592\n",
      "  validation accuracy: \t\t\t15.16 %\n",
      "Evaluating on test data\n",
      "Final results:\n",
      "  test accuracy:\t\t15.96 %\n",
      "We need more magic! Follow instructons below\n",
      "Peak memory usage by Pytorch tensors: 5651.73 Mb\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --bs=128 --num_epochs=1 --evaluate --sched --arrangement_num=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 116.507s\n",
      "  (added) training part takes 107.948s to complete\n",
      "  (added) on average one batch of size 128s is processed in 0.013s\n",
      "  training loss (in-iteration): \t4.669162\n",
      "  validation accuracy: \t\t\t14.52 %\n",
      "Evaluating on test data\n",
      "Final results:\n",
      "  test accuracy:\t\t15.64 %\n",
      "We need more magic! Follow instructons below\n",
      "Peak memory usage by Pytorch tensors: 5494.85 Mb\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --bs=128 --num_epochs=1 --evaluate --sched --arrangement_num=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 116.684s\n",
      "  (added) training part takes 108.135s to complete\n",
      "  (added) on average one batch of size 128s is processed in 0.014s\n",
      "  training loss (in-iteration): \t4.675464\n",
      "  validation accuracy: \t\t\t13.65 %\n",
      "Evaluating on test data\n",
      "Final results:\n",
      "  test accuracy:\t\t14.17 %\n",
      "We need more magic! Follow instructons below\n",
      "Peak memory usage by Pytorch tensors: 5494.85 Mb\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --bs=128 --num_epochs=1 --evaluate --sched --arrangement_num=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 116.729s\n",
      "  (added) training part takes 108.137s to complete\n",
      "  (added) on average one batch of size 128s is processed in 0.014s\n",
      "  training loss (in-iteration): \t4.655618\n",
      "  validation accuracy: \t\t\t15.28 %\n",
      "Evaluating on test data\n",
      "Final results:\n",
      "  test accuracy:\t\t16.06 %\n",
      "We need more magic! Follow instructons below\n",
      "Peak memory usage by Pytorch tensors: 5483.24 Mb\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --bs=128 --num_epochs=1 --evaluate --sched --arrangement_num=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearly, results change from one run to another, but still stay close\n",
    "results = pd.DataFrame()\n",
    "results.loc['accuracy after 1 epoch on test', 'baseline'] = '14.74%'\n",
    "results.loc['batch size', 'baseline'] = '128'\n",
    "results.loc['time per epoch', 'baseline'] = '113.201s'\n",
    "results.loc['Peak memory usage by Pytorch tensors', 'baseline'] = '5651.73 Mb'\n",
    "\n",
    "results.loc['accuracy after 1 epoch on test', 'arrangement_0'] = '16.01%'\n",
    "results.loc['batch size', 'arrangement_0'] = '128'\n",
    "results.loc['time per epoch', 'arrangement_0'] = '113.525s'\n",
    "results.loc['Peak memory usage by Pytorch tensors', 'arrangement_0'] = '5651.73 Mb'\n",
    "\n",
    "results.loc['accuracy after 1 epoch on test', 'arrangement_1'] = '15.17%'\n",
    "results.loc['batch size', 'arrangement_1'] = '128'\n",
    "results.loc['time per epoch', 'arrangement_1'] = '113.517s'\n",
    "results.loc['Peak memory usage by Pytorch tensors', 'arrangement_1'] = '5651.73 Mb'\n",
    "\n",
    "results.loc['accuracy after 1 epoch on test', 'arrangement_2'] = '15.96%'\n",
    "results.loc['batch size', 'arrangement_2'] = '128'\n",
    "results.loc['time per epoch', 'arrangement_2'] = '115.267s'\n",
    "results.loc['Peak memory usage by Pytorch tensors', 'arrangement_2'] = '5651.73 Mb'\n",
    "\n",
    "results.loc['accuracy after 1 epoch on test', 'arrangement_3'] = '15.64%'\n",
    "results.loc['batch size', 'arrangement_3'] = '128'\n",
    "results.loc['time per epoch', 'arrangement_3'] = '116.507s'\n",
    "results.loc['Peak memory usage by Pytorch tensors', 'arrangement_3'] = '5494.85 Mb'\n",
    "\n",
    "results.loc['accuracy after 1 epoch on test', 'arrangement_4'] = '14.17%'\n",
    "results.loc['batch size', 'arrangement_4'] = '128'\n",
    "results.loc['time per epoch', 'arrangement_4'] = '116.684s'\n",
    "results.loc['Peak memory usage by Pytorch tensors', 'arrangement_4'] = '5494.85 Mb'\n",
    "\n",
    "results.loc['accuracy after 1 epoch on test', 'arrangement_5'] = '16.06%'\n",
    "results.loc['batch size', 'arrangement_5'] = '128'\n",
    "results.loc['time per epoch', 'arrangement_5'] = '116.729s'\n",
    "results.loc['Peak memory usage by Pytorch tensors', 'arrangement_5'] = '5483.24 Mb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>arrangement_0</th>\n",
       "      <th>arrangement_1</th>\n",
       "      <th>arrangement_2</th>\n",
       "      <th>arrangement_3</th>\n",
       "      <th>arrangement_4</th>\n",
       "      <th>arrangement_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy after 1 epoch on test</th>\n",
       "      <td>14.74%</td>\n",
       "      <td>16.01%</td>\n",
       "      <td>15.17%</td>\n",
       "      <td>15.96%</td>\n",
       "      <td>15.64%</td>\n",
       "      <td>14.17%</td>\n",
       "      <td>16.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch size</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time per epoch</th>\n",
       "      <td>113.201s</td>\n",
       "      <td>113.525s</td>\n",
       "      <td>113.517s</td>\n",
       "      <td>115.267s</td>\n",
       "      <td>116.507s</td>\n",
       "      <td>116.684s</td>\n",
       "      <td>116.729s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peak memory usage by Pytorch tensors</th>\n",
       "      <td>5651.73 Mb</td>\n",
       "      <td>5651.73 Mb</td>\n",
       "      <td>5651.73 Mb</td>\n",
       "      <td>5651.73 Mb</td>\n",
       "      <td>5494.85 Mb</td>\n",
       "      <td>5494.85 Mb</td>\n",
       "      <td>5483.24 Mb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        baseline arrangement_0 arrangement_1  \\\n",
       "accuracy after 1 epoch on test            14.74%        16.01%        15.17%   \n",
       "batch size                                   128           128           128   \n",
       "time per epoch                          113.201s      113.525s      113.517s   \n",
       "Peak memory usage by Pytorch tensors  5651.73 Mb    5651.73 Mb    5651.73 Mb   \n",
       "\n",
       "                                     arrangement_2 arrangement_3  \\\n",
       "accuracy after 1 epoch on test              15.96%        15.64%   \n",
       "batch size                                     128           128   \n",
       "time per epoch                            115.267s      116.507s   \n",
       "Peak memory usage by Pytorch tensors    5651.73 Mb    5494.85 Mb   \n",
       "\n",
       "                                     arrangement_4 arrangement_5  \n",
       "accuracy after 1 epoch on test              14.17%        16.06%  \n",
       "batch size                                     128           128  \n",
       "time per epoch                            116.684s      116.729s  \n",
       "Peak memory usage by Pytorch tensors    5494.85 Mb    5483.24 Mb  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that as we checkpoint more layers of the network, the memoery consumption decreases. I must say that I expected it to decrease even more, so there might be a mistake somewhere in my code. Still, it is quite nice to see.\n",
    "\n",
    "Baseline peak memory consumption is `5651 Mb` while the setting where all layers are checkpointed (apart from Batch Norm and Dropout) the peak memory consumption is `5483.24 Mb`, so we won about `200 Mb`. I wonder if I checkpointed the whole model, what would be the tradeoff between accuracy/wrong calculations in the BN and dropout layer and gains in memory consumption.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjY8LR_GQbTV"
   },
   "source": [
    "### Accumulating gradient for large batches (3 points)\n",
    "We can increase the effective batch size by simply accumulating gradients over multiple forward passes. Note that `loss.backward()` simply adds the computed gradient to `tensor.grad`, so we can call this method multiple times before actually taking an optimizer step. However, this approach might be a little tricky to combine with batch normalization. Do you see why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "I think accumulating gradient for large batches is tricky to combine with batch normalization for the following reason. \n",
    "\n",
    "From what I've read, running mean and variance are calculated each forward pass.\n",
    "Then, it follows that running mean and variance will be updated for each batch, and it is different from calculating mean/variance for the `effective batch`. Simply said, updating running mean and variance 32 times for a batch size of 32 is different from updating running mean and variance from one batch of 1024 examples. Therefore, we recalculate mean and variance more frequently than update the parameters of the BN layer in the backward pass.\n",
    "\n",
    "I guess the problem becomes clearer if we think of calculating `mean/variance` from a batch of 2 images (imagine a huge model), and `mean/variance` from, say, 10 images. The latter might be more stable and lead to faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qbbbO7V0QeGT"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-939610cc10cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatches_per_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meffective_batch_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mloader_batch_size\u001b[0m \u001b[0;31m# Updating weights after 8 forward passes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "effective_batch_size = 1024\n",
    "loader_batch_size = 32\n",
    "batches_per_update = effective_batch_size / loader_batch_size # Updating weights after 8 forward passes\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=loader_batch_size)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for batch_i, (batch_X, batch_y) in enumerate(dataloader):\n",
    "    l = loss(model(batch_X), batch_y)\n",
    "    l.backward() # Adds gradients\n",
    "  \n",
    "    if (batch_i + 1) % batches_per_update == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 111.352s\n",
      "  (added) training part takes 102.778s to complete\n",
      "  (added) on average one batch of size 128s is processed in 0.012s\n",
      "  training loss (in-iteration): \t4.855658\n",
      "  validation accuracy: \t\t\t11.21 %\n",
      "Evaluating on test data\n",
      "Final results:\n",
      "  test accuracy:\t\t11.29 %\n",
      "We need more magic! Follow instructons below\n",
      "Peak memory usage by Pytorch tensors: 5651.73 Mb\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --bs=128 --num_epochs=1 --evaluate --sched --ebs=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqxvZWH9Uxtq"
   },
   "source": [
    "**Task**. Explore the trade-off between computation time and memory usage while maintaining the same effective batch size. By effective batch size we mean the number of objects over which the loss is computed before taking a gradient step.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* Compare compute between accumulating gradient and gradient checkpointing with similar memory consumptions\n",
    "* Incorporate gradient accumulation into your script with optional argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that accumulating gradient does not reduce memory consumption when compared with the baseline model or checkpointing, when the batch size is actually as large as used in the baseline model (in my case, `batch size = 128`. I think it makes sense, because in the code the only change is that we make a step only once in `effective_batch_size / loader_batch_size` forward passes. \n",
    "\n",
    "However, when it start to be interesting and important, and that's the idea of accumulating gradients is when we use a lower batch size, than used in the baseline model. See below, when I changed to batch size of 64, but still effective_batch_size = 1024. \n",
    "\n",
    "It becomes important when training huge models when, for example, only one image can fit into batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 107.471s\n",
      "  (added) training part takes 99.365s to complete\n",
      "  (added) on average one batch of size 64s is processed in 0.034s\n",
      "  training loss (in-iteration): \t4.868647\n",
      "  validation accuracy: \t\t\t10.98 %\n",
      "Evaluating on test data\n",
      "Final results:\n",
      "  test accuracy:\t\t11.13 %\n",
      "We need more magic! Follow instructons below\n",
      "Peak memory usage by Pytorch tensors: 3064.78 Mb\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --bs=64 --num_epochs=1 --evaluate --sched --ebs=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3iiJZuhSUR0"
   },
   "source": [
    "## Accuracy vs compute trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXad1svpSk8f"
   },
   "source": [
    "### Knowledge distillation (6 points)\n",
    "Suppose that we have a large network (*teacher network*) or an ensemble of networks which has a good accuracy. We can like train a much smaller network (*student network*) using the outputs of teacher networks. It turns out that the perfomance could be even better! This approach doesn't help with training speed, but can be quite beneficial when we'd like to reduce the model size for low-memory devices.\n",
    "\n",
    "* https://www.ttic.edu/dl/dark14.pdf\n",
    "* [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n",
    "* https://medium.com/neural-machines/knowledge-distillation-dc241d7c2322\n",
    "\n",
    "Even the completely different ([article](https://arxiv.org/abs/1711.10433)) architecture can be used in a student model, e.g. you can approximate an autoregressive model (WaveNet) by a non-autoregressive one.\n",
    "\n",
    "**Task:** \n",
    "1. Train good enough (teacher) network, achieve >=35% accuracy on validation set.\n",
    "2. Train small (student) network, achieve 20-25% accuracy, draw a plot \"training and testing errors vs train step index\"\n",
    "3. Distill teacher network with student network, achieve at least +1% improvement in accuracy over student network accuracy.\n",
    "\n",
    "_Please, don't cheat with early-early-early stopping while training of the student network. Make sure, it  converged._\n",
    "\n",
    "**Note**. Logits carry more information than the probabilities after softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import skimage\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Train small (student) network, achieve 20-25% accuracy, draw a plot \"training and testing errors vs train step index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', transform=transform_train)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [80000, 20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I like about the student model below is that it is rather simple: there is only one convolutional layer + maxpooling followed by fully-connected layers with some batch-normalization layers and ReLU non-linearities. This allows to iterate very quickly, one epoch takes about 33 seconds. At the same time, accuracy of about 20-22% is reached after 4-5 epochs. Training more does not improve accuracy, so it is possible to claim that we trained a student network.\n",
    "\n",
    "I train it without data augmentation, so as to see the pure result of training the model and distilling the knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module('conv1', nn.Conv2d(3, 100, kernel_size=(3,3), stride=1))\n",
    "model.add_module('bn1', nn.BatchNorm2d(100))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('maxpool1', nn.MaxPool2d(3))\n",
    "\n",
    "model.add_module('flatten', nn.Flatten())\n",
    "model.add_module('fc1', nn.Linear(40000, 1000))\n",
    "model.add_module('bn2', nn.BatchNorm1d(1000))\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('dp1', nn.Dropout(0.5))\n",
    "model.add_module('fc2', nn.Linear(1000, 200))\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#L2 regularization is added through weight_decay\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs, scheduler = None):\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train(True)\n",
    "        start_time = time.time()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            \n",
    "            # I. Training\n",
    "            #1 Train on GPU\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            #2 Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #3 Forward\n",
    "            predictions = model.forward(x_batch)\n",
    "            \n",
    "            #4 Calculating loss\n",
    "            loss = loss_fn(predictions, y_batch)\n",
    "            \n",
    "            #5 Calculating gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #6 Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # II. Tracking the training\n",
    "            train_loss.append(loss.cpu().data.numpy())\n",
    "            y_pred = predictions.max(1)[1].data\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        # III. Validation\n",
    "        model.train(False) # disable dropout / use averages for batch_norm\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            predictions = model.forward(x_batch)\n",
    "            y_pred = predictions.max(1)[1].data\n",
    "            val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "        \n",
    "        validation_accuracy = np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100\n",
    "        val_accuracy_per_epoch.append(validation_accuracy)\n",
    "        # IV. Reporting\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "            np.mean(train_loss[-len(train_dataset) // batch_size :])))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "            validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 30 took 33.374s\n",
      "  training loss (in-iteration): \t4.610269\n",
      "  validation accuracy: \t\t\t13.88 %\n",
      "Epoch 2 of 30 took 33.431s\n",
      "  training loss (in-iteration): \t4.000124\n",
      "  validation accuracy: \t\t\t17.53 %\n",
      "Epoch 3 of 30 took 33.472s\n",
      "  training loss (in-iteration): \t3.630038\n",
      "  validation accuracy: \t\t\t19.14 %\n",
      "Epoch 4 of 30 took 33.449s\n",
      "  training loss (in-iteration): \t3.330232\n",
      "  validation accuracy: \t\t\t20.39 %\n",
      "Epoch 5 of 30 took 33.481s\n",
      "  training loss (in-iteration): \t3.051471\n",
      "  validation accuracy: \t\t\t20.23 %\n",
      "Epoch 6 of 30 took 33.259s\n",
      "  training loss (in-iteration): \t2.770838\n",
      "  validation accuracy: \t\t\t20.85 %\n",
      "Epoch 7 of 30 took 33.640s\n",
      "  training loss (in-iteration): \t2.500934\n",
      "  validation accuracy: \t\t\t20.76 %\n",
      "Epoch 8 of 30 took 33.469s\n",
      "  training loss (in-iteration): \t2.229225\n",
      "  validation accuracy: \t\t\t20.27 %\n",
      "Epoch 9 of 30 took 33.775s\n",
      "  training loss (in-iteration): \t1.963818\n",
      "  validation accuracy: \t\t\t19.77 %\n",
      "Epoch 10 of 30 took 33.619s\n",
      "  training loss (in-iteration): \t1.704719\n",
      "  validation accuracy: \t\t\t19.81 %\n",
      "Epoch 11 of 30 took 33.322s\n",
      "  training loss (in-iteration): \t1.482306\n",
      "  validation accuracy: \t\t\t19.26 %\n",
      "Epoch 12 of 30 took 33.450s\n",
      "  training loss (in-iteration): \t1.276931\n",
      "  validation accuracy: \t\t\t19.02 %\n",
      "Epoch 13 of 30 took 33.495s\n",
      "  training loss (in-iteration): \t1.108192\n",
      "  validation accuracy: \t\t\t18.59 %\n",
      "Epoch 14 of 30 took 33.127s\n",
      "  training loss (in-iteration): \t0.958719\n",
      "  validation accuracy: \t\t\t18.95 %\n",
      "Epoch 15 of 30 took 33.559s\n",
      "  training loss (in-iteration): \t0.843337\n",
      "  validation accuracy: \t\t\t18.23 %\n",
      "Epoch 16 of 30 took 33.333s\n",
      "  training loss (in-iteration): \t0.526464\n",
      "  validation accuracy: \t\t\t19.41 %\n",
      "Epoch 17 of 30 took 33.315s\n",
      "  training loss (in-iteration): \t0.451838\n",
      "  validation accuracy: \t\t\t19.32 %\n",
      "Epoch 18 of 30 took 33.258s\n",
      "  training loss (in-iteration): \t0.409859\n",
      "  validation accuracy: \t\t\t19.10 %\n",
      "Epoch 19 of 30 took 33.179s\n",
      "  training loss (in-iteration): \t0.383893\n",
      "  validation accuracy: \t\t\t19.11 %\n",
      "Epoch 20 of 30 took 33.196s\n",
      "  training loss (in-iteration): \t0.358161\n",
      "  validation accuracy: \t\t\t18.81 %\n",
      "Epoch 21 of 30 took 33.199s\n",
      "  training loss (in-iteration): \t0.337331\n",
      "  validation accuracy: \t\t\t19.10 %\n",
      "Epoch 22 of 30 took 33.427s\n",
      "  training loss (in-iteration): \t0.321815\n",
      "  validation accuracy: \t\t\t19.23 %\n",
      "Epoch 23 of 30 took 33.681s\n",
      "  training loss (in-iteration): \t0.302339\n",
      "  validation accuracy: \t\t\t18.88 %\n",
      "Epoch 24 of 30 took 33.271s\n",
      "  training loss (in-iteration): \t0.287655\n",
      "  validation accuracy: \t\t\t18.71 %\n",
      "Epoch 25 of 30 took 33.288s\n",
      "  training loss (in-iteration): \t0.273644\n",
      "  validation accuracy: \t\t\t18.84 %\n",
      "Epoch 26 of 30 took 33.374s\n",
      "  training loss (in-iteration): \t0.259133\n",
      "  validation accuracy: \t\t\t18.72 %\n",
      "Epoch 27 of 30 took 33.237s\n",
      "  training loss (in-iteration): \t0.250216\n",
      "  validation accuracy: \t\t\t18.82 %\n",
      "Epoch 28 of 30 took 33.635s\n",
      "  training loss (in-iteration): \t0.234367\n",
      "  validation accuracy: \t\t\t18.60 %\n",
      "Epoch 29 of 30 took 33.256s\n",
      "  training loss (in-iteration): \t0.224120\n",
      "  validation accuracy: \t\t\t18.40 %\n",
      "Epoch 30 of 30 took 33.500s\n",
      "  training loss (in-iteration): \t0.215863\n",
      "  validation accuracy: \t\t\t18.58 %\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_accuracy = []\n",
    "val_accuracy_per_epoch = []\n",
    "train_model(model, train_loader, val_loader, optimizer, 30, scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained for 30 epochs just to show that accuracy of this student model does not exceed 21% on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [(i+1)*len(train_dataset)/batch_size for i in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('training step')\n",
    "ax1.set_ylabel('cross-entropy loss', color=color)\n",
    "ax1.plot(train_loss, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('validation accuracy per epoch', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(x,val_accuracy_per_epoch, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Distill teacher network with student network, achieve at least +1% improvement in accuracy over student network accuracy.\n",
    "\n",
    "First, I set up the model as before, but I will train it using the predictions coming from the \"cumbersome\" model trained and saved in `homework_part2.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = torch.load('cumbersome_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define new loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_with_soft_targets(pred, soft_targets):\n",
    "    logsoftmax = nn.LogSoftmax(dim=1)\n",
    "    return torch.mean(torch.sum(- soft_targets * logsoftmax(pred), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossTemperature_withSoftTargets(torch.nn.Module):\n",
    "    def __init__(self, temperature, reduction='mean'):\n",
    "        super(CrossEntropyLossTemperature_withSoftTargets, self).__init__()\n",
    "        self.T = temperature\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, soft_targets, hard_targets):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        z = input / self.T\n",
    "        loss_1 = self.T**2*cross_entropy_with_soft_targets(z, F.softmax(soft_targets, 1, _stacklevel=5))\n",
    "        loss_2 = F.cross_entropy(input, hard_targets, weight=None,\n",
    "                               ignore_index=-100, reduction=self.reduction)\n",
    "        \n",
    "        return loss_1+loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = nn.Sequential()\n",
    "\n",
    "student.add_module('conv1', nn.Conv2d(3, 100, kernel_size=(3,3), stride=1))\n",
    "student.add_module('bn1', nn.BatchNorm2d(100))\n",
    "student.add_module('relu1', nn.ReLU())\n",
    "student.add_module('maxpool1', nn.MaxPool2d(3))\n",
    "\n",
    "student.add_module('flatten', nn.Flatten())\n",
    "student.add_module('fc1', nn.Linear(40000, 1000))\n",
    "student.add_module('bn2', nn.BatchNorm1d(1000))\n",
    "student.add_module('relu2', nn.ReLU())\n",
    "student.add_module('dp1', nn.Dropout(0.5))\n",
    "student.add_module('fc2', nn.Linear(1000, 200))\n",
    "\n",
    "student = student.to(device)\n",
    "loss_fn = CrossEntropyLossTemperature_withSoftTargets(1)\n",
    "\n",
    "#L2 regularization is added through weight_decay\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=0.0001, weight_decay=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_knowledge(teacher,val_accuracy\n",
    "                      student,\n",
    "                      train_loader,\n",
    "                      val_loader,\n",
    "                      optimizer,\n",
    "                      num_epochs,\n",
    "                      scheduler = None):\n",
    "\n",
    "    teacher.train(False)   \n",
    "    for epoch in range(num_epochs):\n",
    "        student.train(True) \n",
    "        start_time = time.time()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            \n",
    "            # I. Training\n",
    "            #1 Train on GPU\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            #2 Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #3 Forward\n",
    "            predictions_teacher = teacher.forward(x_batch)\n",
    "            predictions_student = student.forward(x_batch)\n",
    "            \n",
    "            #4 Calculating loss\n",
    "            loss = loss_fn(predictions_student, predictions_teacher, y_batch)\n",
    "            \n",
    "            #5 Calculating gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #6 Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # II. Tracking the training\n",
    "            train_loss.append(loss.cpu().data.numpy())  \n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        # III. Validation\n",
    "        student.train(False) # disable dropout / use averages for batch_norm\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            predictions_student = student.forward(x_batch)\n",
    "            y_pred = predictions_student.max(1)[1].data\n",
    "            val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "        \n",
    "        validation_accuracy = np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100\n",
    "        # IV. Reporting\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "            np.mean(train_loss[-len(train_dataset) // batch_size :])))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "            validation_accuracy))\n",
    "        \n",
    "        if validation_accuracy > 24:\n",
    "            print(f'Fitted the model to exceed 24% on the validation set.Exiting loop on epoch {epoch + 1}.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 123.992s\n",
      "  training loss (in-iteration): \t8.938258\n",
      "  validation accuracy: \t\t\t16.54 %\n",
      "Epoch 2 of 100 took 124.184s\n",
      "  training loss (in-iteration): \t7.694273\n",
      "  validation accuracy: \t\t\t20.11 %\n",
      "Epoch 3 of 100 took 124.379s\n",
      "  training loss (in-iteration): \t7.056781\n",
      "  validation accuracy: \t\t\t21.85 %\n",
      "Epoch 4 of 100 took 124.174s\n",
      "  training loss (in-iteration): \t6.610803\n",
      "  validation accuracy: \t\t\t23.08 %\n",
      "Epoch 5 of 100 took 124.029s\n",
      "  training loss (in-iteration): \t6.226841\n",
      "  validation accuracy: \t\t\t23.47 %\n",
      "Epoch 6 of 100 took 124.229s\n",
      "  training loss (in-iteration): \t5.899975\n",
      "  validation accuracy: \t\t\t23.86 %\n",
      "Epoch 7 of 100 took 124.178s\n",
      "  training loss (in-iteration): \t5.591008\n",
      "  validation accuracy: \t\t\t23.30 %\n",
      "Epoch 8 of 100 took 124.259s\n",
      "  training loss (in-iteration): \t5.309193\n",
      "  validation accuracy: \t\t\t23.96 %\n",
      "Epoch 9 of 100 took 123.999s\n",
      "  training loss (in-iteration): \t5.057131\n",
      "  validation accuracy: \t\t\t23.83 %\n",
      "Epoch 10 of 100 took 123.924s\n",
      "  training loss (in-iteration): \t4.843023\n",
      "  validation accuracy: \t\t\t23.66 %\n",
      "Epoch 11 of 100 took 124.313s\n",
      "  training loss (in-iteration): \t4.651734\n",
      "  validation accuracy: \t\t\t23.34 %\n",
      "Epoch 12 of 100 took 124.098s\n",
      "  training loss (in-iteration): \t4.491565\n",
      "  validation accuracy: \t\t\t23.30 %\n",
      "Epoch 13 of 100 took 124.028s\n",
      "  training loss (in-iteration): \t4.351459\n",
      "  validation accuracy: \t\t\t23.68 %\n",
      "Epoch 14 of 100 took 124.000s\n",
      "  training loss (in-iteration): \t4.231526\n",
      "  validation accuracy: \t\t\t23.56 %\n",
      "Epoch 15 of 100 took 124.084s\n",
      "  training loss (in-iteration): \t4.138215\n",
      "  validation accuracy: \t\t\t23.34 %\n",
      "Epoch 16 of 100 took 124.380s\n",
      "  training loss (in-iteration): \t3.806899\n",
      "  validation accuracy: \t\t\t24.08 %\n",
      "Fitted the model to exceed 24% on the validation set.Exiting loop on epoch 15.\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_accuracy = []\n",
    "distill_knowledge(teacher, student, train_loader, val_loader, optimizer, 100, scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the results are pretty amazing. To be sure, let's check on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root_folder, labels_frame, class_to_idx, transform=None):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.root_folder = root_folder\n",
    "        self.labels_frame = labels_frame\n",
    "        self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame) - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.root_folder,\n",
    "                                self.labels_frame.loc[idx, 'imname'])\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        # Treating greyscale images\n",
    "        if len(image.shape) < 3:\n",
    "            image = skimage.color.grey2rgb(image, alpha=None)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        category = self.class_to_idx[self.labels_frame.loc[idx, 'id']]\n",
    "        return image, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t23.98 %\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_csv('tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None)\n",
    "labels.columns = ['imname', 'id', 'bb1', 'bb2', 'bb3', 'bb4']\n",
    "class_to_idx = dataset.class_to_idx\n",
    "test_dataset = TestDataset(root_folder='tiny-imagenet-200/val/images/',\n",
    "                           labels_frame=labels,\n",
    "                           class_to_idx=class_to_idx,\n",
    "                           transform = transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "                           ]))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                            batch_size=10,\n",
    "                            shuffle=True,\n",
    "                            num_workers=1)\n",
    "\n",
    "student.train(False)\n",
    "correct_samples = 0\n",
    "total_samples = 0\n",
    "for x_batch, y_batch in test_loader:\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "    predictions = student.forward(x_batch)\n",
    "    y_pred = predictions.max(1)[1].data\n",
    "    correct_samples += torch.sum(y_pred == y_batch)\n",
    "    total_samples += y_batch.shape[0]\n",
    "test_accuracy = float(correct_samples) / total_samples\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TL;DR. Moar techniques on accuracy vs time trade-off (just for your information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WOWhqMJSboR"
   },
   "source": [
    "### Tensor type size\n",
    "\n",
    "One of the hyperparameter affecting memory consumption is the precision (e.g. floating point number). The most popular choice is 32 bit however with several hacks* 16 bit arithmetics can save you approximately half of the memory without considerable loss of perfomance. This is called mixed precision training.\n",
    "\n",
    "*https://arxiv.org/pdf/1710.03740.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xAEF9aJc-43"
   },
   "source": [
    "### Quantization\n",
    "\n",
    "We can actually move further and use even lower precision like 8-bit integers:\n",
    "\n",
    "* https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd\n",
    "* https://nervanasystems.github.io/distiller/quantization/\n",
    "* https://arxiv.org/abs/1712.05877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "The idea of pruning is to remove unnecessary (in terms of loss) weights. It can be measured in different ways: for example, by the norm of the weights (similar to L1 feature selection), by the magnitude of the activation or via Taylor expansion*.\n",
    "\n",
    "One iteration of pruning consists of two steps:\n",
    "\n",
    "1) Rank weights with some importance measure and remove the least important\n",
    "\n",
    "2) Fine-tune the model\n",
    "\n",
    "This approach is a bit computationally heavy but can lead to drastic (up to 150x) decrease of memory to store the weights. Moreover if you make use of structure in layers you can decrease also compute. For example, the whole convolutional filters can be removed.\n",
    "\n",
    "*https://arxiv.org/pdf/1611.06440.pdf"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework_optimization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
